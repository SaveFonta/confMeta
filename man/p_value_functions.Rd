% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pfun_edgington.R, R/pfun_fisher.R,
%   R/pfun_hmean.R, R/pfun_wilkinson.R, R/pfun_pearson.R, R/pfun_tippett.R,
%   R/pfun_stouffer.R, R/pfun_edgington_w.R
\name{p_edgington}
\alias{p_edgington}
\alias{p_fisher}
\alias{p_hmean}
\alias{p_wilkinson}
\alias{p_pearson}
\alias{p_tippett}
\alias{p_stouffer}
\alias{p_edgington_w}
\title{\emph{p}-value functions}
\usage{
p_edgington(
  estimates,
  SEs,
  mu = 0,
  heterogeneity = "none",
  phi = NULL,
  tau2 = NULL,
  alternative = "two.sided",
  check_inputs = TRUE,
  approx = TRUE,
  input_p = "greater"
)

p_fisher(
  estimates,
  SEs,
  mu = 0,
  phi = NULL,
  tau2 = NULL,
  heterogeneity = "none",
  check_inputs = TRUE,
  input_p = "greater"
)

p_hmean(
  estimates,
  SEs,
  mu = 0,
  phi = NULL,
  tau2 = NULL,
  heterogeneity = "none",
  alternative = "none",
  check_inputs = TRUE,
  w = rep(1, length(estimates)),
  distr = "chisq"
)

p_wilkinson(
  estimates,
  SEs,
  mu,
  phi = NULL,
  tau2 = NULL,
  heterogeneity = "none",
  alternative = "none",
  check_inputs = TRUE,
  input_p = "greater"
)

p_pearson(
  estimates,
  SEs,
  mu = 0,
  phi = NULL,
  tau2 = NULL,
  heterogeneity = "none",
  check_inputs = TRUE,
  input_p = "greater"
)

p_tippett(
  estimates,
  SEs,
  mu = 0,
  phi = NULL,
  tau2 = NULL,
  heterogeneity = "none",
  check_inputs = TRUE,
  input_p = "greater"
)

p_stouffer(
  estimates,
  SEs,
  mu = 0,
  phi = NULL,
  tau2 = NULL,
  heterogeneity = "none",
  alternative = "two.sided",
  check_inputs = TRUE,
  w = NULL
)

p_edgington_w(
  estimates,
  SEs,
  mu = 0,
  alternative = "two.sided",
  approx = TRUE,
  input_p = "greater",
  w = rep(1, length(estimates)),
  heterogeneity = "none",
  phi = NULL,
  tau2 = NULL,
  approx_rule = "neff",
  neff_cut = 12,
  check_inputs = TRUE
)
}
\arguments{
\item{estimates}{Numeric vector of study-level effect estimates.}

\item{SEs}{Numeric vector of corresponding standard errors.}

\item{mu}{Numeric scalar or vector of null values for the overall effect
(default: 0). The function is vectorized over \code{mu}.}

\item{heterogeneity}{Character string: \code{"none"} (default),
\code{"additive"}, or \code{"multiplicative"}. Determines whether
standard errors are adjusted for between-study heterogeneity using
\code{tau2} or \code{phi}.}

\item{phi}{Multiplicative heterogeneity parameter (if applicable).}

\item{tau2}{Additive heterogeneity parameter (if applicable).}

\item{alternative}{Either \code{"two.sided"} (default), or \code{"one.sided"}.
\strong{Currently has no effect on the output}: all results are converted
to two-sided combined \emph{p}-values, regardless of the argument.
The argument is included only for consistency with other functions (since this function is a blueprint of p_edgington)
and for input validation.}

\item{check_inputs}{Logical (default \code{TRUE}). If \code{TRUE},
perform input validation.}

\item{approx}{Logical (default \code{TRUE}). If \code{TRUE}, use a normal
approximation for the weighted sum when the condition defined by \code{approx_rule}
is met}

\item{input_p}{Type of study-level \emph{p}-values used in the combination:
\code{"greater"}, \code{"less"}, or \code{"two.sided"}.
If \code{"greater"} or \code{"less"}, one-sided \emph{p}-values are
first computed, then symmetrized to two-sided in the output.}

\item{w}{Numeric vector of nonnegative weights, same length as \code{estimates}.
Defaults to equal weights.}

\item{distr}{The distribution to use for the calculation of the p-value. Currently, the options are
\code{"f"} (F-distribution) and \code{"chisq"} (Chi-squared distribution). Defaults to \code{"chisq"}.}

\item{approx_rule}{Rule for normal approximation: \code{"n"}
uses the number of studies; \code{"neff"} (default) uses the effective sample
size criterion (see Details).}

\item{neff_cut}{Numeric threshold (default 12). If \code{approx_rule="n"},
normal approximation is used when \eqn{n \geq 12}. If
\code{approx_rule="neff"}, normal approximation is used when
\eqn{\|w\|_2^4 / \|w\|_4^4 \geq 12}.}
}
\value{
The corresponding p-values given \eqn{mu} under the null-hypothesis.

A numeric vector of combined \emph{p}-values corresponding
to each value of \code{mu}. Note that the output is only
symmetrized to a two-sided \emph{p}-value if \code{input_p} is
\code{"greater"} or \code{"less"}.

@references
D. L. Barrow and P. W. Smith. Spline notation applied to a volume
problem. \emph{The American Mathematical Monthly}, 86(1):50-51, 1979.

H. Cramér. \emph{Mathematical Methods of Statistics}. Princeton University
Press, 1946.

E. G. Olds. A note on the convolution of uniform distributions.
\emph{The Annals of Mathematical Statistics}, 23(2):282-285, 1952.

B. D. Ripley. \emph{Stochastic Simulation}. John Wiley & Sons, Hoboken,
NJ, 1987.

Add others on p value functions
}
\description{
\if{html}{\out{<div class="sourceCode">}}\preformatted{These functions combine individual effect estimates and the corresponding
standard errors into a single \\emph\{p\}-value. Under the hood, all of the
functions transform the estimates and standard errors into \\emph\{z\}, and
subsequently into \\emph\{p\}-values. The resulting \\emph\{p\}-values are
combined into the chosen statistic and an appropriate distribution is
used to derive the combined \\emph\{p\}-value.

All of the \\emph\{p\}-value functions are vectorized over the \code{mu}
argument.
}\if{html}{\out{</div>}}

Weighted generalization of Edgington’s method for combining
\emph{p}-values across studies. The method forms a weighted sum
of individual study \emph{p}-values and evaluates it against the
exact or approximate null distribution.

If all weights are equal, this reduces to the classical Edgington
procedure, where the null distribution is given by the Irwin–Hall distribution.
}
\details{
The weighted Edgington statistic is defined as
\deqn{S = \sum_{i=1}^k w_i p_i,}
where \eqn{w_i} are positive study weights and \eqn{p_i} are individual
study \emph{p}-values. Under the global null hypothesis, each \eqn{p_i}
is assumed to follow a \eqn{Unif(0, 1)} distribution.
}
\note{
Add references to p-value statistics.
}
\section{Null Distribution and Approximation}{

The CDF of the test statistic S under the null, \eqn{F(t) = P(S \leq t)},
is computed in one of two ways:
\itemize{
\item \strong{Exact Method:} The function uses the exact Barrow-Smith
inclusion-exclusion formula to compute the CDF.
This method is computationally intensive and is infeasible for \code{n > 18}
studies, at which point the function will stop with an error if
\code{approx = FALSE} is used.

\item \strong{Normal Approximation:} For a large number of studies or
sufficiently balanced weights, S is approximated by a Normal
distribution  with:
\deqn{E[S] = \frac{1}{2}\sum_{i=1}^k w_i}
\deqn{Var(S) = \frac{1}{12}\sum_{i=1}^k w_i^2}
}
}

\section{Approximation Rule}{

The \code{approx = TRUE} argument enables the normal approximation, but it is
only used if a condition (controlled by \code{approx_rule}) is met.
\itemize{
\item \code{approx_rule = "n"}: Uses the approximation if the
number of studies n > neff_cut.
\item \code{approx_rule = "neff"} (default): Uses the approximation if the
\strong{effective sample size} n_eff > neff_cut.
}
The effective sample size is defined as:
\deqn{n_{\mathrm{eff}} = \frac{(\sum w_i^2)^2}{\sum w_i^4} = \frac{\|w\|_2^4}{\|w\|_4^4}}
The default threshold \code{neff_cut = 12} is based on error bounds, which indicate the approximation is
sufficiently accurate when this condition is met. This
rule is more robust than \code{approx_rule = "n"} when weights are
unbalanced.
}

\section{Approximation Error}{

The \code{neff_cut} parameter directly controls the tolerance for
the approximation error.
\strong{Edgeworth Approximation:} This provides an
\emph{approximation} of the error, which directly relates to the
\eqn{n_{\mathrm{eff}}} criterion used in this function:
\deqn{\sup_{t \in \mathbb{R}} |F_n^w(t) - \Phi(t)| \approx \frac{||\phi^{(3)}||_{\infty}}{20}\frac{||w||_{4}^{4}}{||w||_{2}^{4}} \approx \frac{0.028}{n_{\mathrm{eff}}}}

The default \code{neff_cut = 12} is chosen based on this, as it
corresponds to an approximate maximum error of
\eqn{0.028 / 12 \approx 0.0023}.
If you change \code{neff_cut}, you are changing this error tolerance.
}

\section{Output p-value}{

The final output depends on the \code{input_p} argument:
\itemize{
\item If \code{input_p} is \code{"greater"} or \code{"less"}, the
input \eqn{p_i} are one-sided. The function computes the
one-sided \emph{p}-value \eqn{sp = P(S \leq s_{\mathrm{obs}})} and
returns a \strong{symmetrized, two-sided \emph{p}-value}:
\eqn{p_{2s} = 2 \min(sp, 1-sp)}.
\item If \code{input_p} is \code{"two.sided"}, the input \eqn{p_i}
are two-sided. The function returns \eqn{sp = P(S \leq s_{\mathrm{obs}})}
\strong{directly, without symmetrization}. Note that summing two-sided
\emph{p}-values is not a standard application of Edgington's method.
}
}

\examples{
    # Simulating estimates and standard errors
    n <- 15
    estimates <- rnorm(n)
    SEs <- rgamma(n, 5, 5)

    # Calculate the between-study variance tau2
    tau2 <- estimate_tau2(estimates = estimates, SEs = SEs)
    phi <- estimate_phi(estimates = estimates, SEs = SEs)

    # Set up a vector of means under the null hypothesis
    mu <- seq(
      min(estimates) - 0.5 * max(SEs),
      max(estimates) + 0.5 * max(SEs),
      length.out = 1e5
    )

    # Using Edgington's method to calculate the combined p-value
    # for each of the means with additive adjustement for SEs
    p_edgington(
        estimates = estimates,
        SEs = SEs,
        mu = mu,
        heterogeneity = "additive",
        tau2 = tau2
    )
    # Using Fisher's method to calculate the combined \emph{p}-value
    # for each of the means with multiplicative adjustement for SEs
    p_fisher(
        estimates = estimates,
        SEs = SEs,
        mu = mu,
        heterogeneity = "multiplicative",
        phi = phi
    )

    # Using the harmonic mean method to calculate the combined p-value
    # for each of the means with additive adjustment for SEs.
    p_hmean(
        estimates = estimates,
        SEs = SEs,
        mu = mu,
        heterogeneity = "additive",
        tau2 = tau2,
        distr = "chisq"
    )

# Using Wilkinson's method to calculate the combined p-value
# for each of the means with multiplicative adjustement for SEs
p_wilkinson(
    estimates = estimates,
    SEs = SEs,
    mu = mu,
    heterogeneity = "multiplicative",
    phi = phi
)
# Using Pearson's method to calculate the combined p-value
# for each of the means with multiplicative adjustement for SEs
p_pearson(
    estimates = estimates,
    SEs = SEs,
    mu = mu,
    heterogeneity = "multiplicative",
    phi = phi
)
# Using Tippett's method to calculate the combined p-value
# for each of the means with multiplicative adjustement for SEs
p_tippett(
    estimates = estimates,
    SEs = SEs,
    mu = mu,
    heterogeneity = "multiplicative",
    phi = phi
)
# Using weighted Stouffer's method to calculate the combined p-value for
# each of the means with multiplicative adjustement for SEs
p_stouffer(
    estimates = estimates,
    SEs = SEs,
    mu = mu,
    heterogeneity = "multiplicative",
    phi = phi
)
}
